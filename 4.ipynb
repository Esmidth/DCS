{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from consts import sensor_day_seconds as day_second\n",
    "from consts import sensor_idd as idd\n",
    "from consts import sensor_idds_uniq as idds_uniq\n",
    "from consts import sensor_idds_uniq_sorted as idds_uniq_sorted\n",
    "from consts import sensor_a_temperature as a_temperature\n",
    "from consts import sensor_s_temperature as s_temperature\n",
    "from consts import sensor_unix_column as unix_column\n",
    "from consts import sensor_idd as idd\n",
    "epoch_length = 30\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "input_file = open('dict_list3_1.pkl','rb')\n",
    "input_list = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esmidth/.pyenv/versions/miniconda3-4.7.12/envs/ccs_ta/lib/python3.7/site-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# 将epoch epoch_len epoch_time 都flat化\n",
    "len_list = []\n",
    "time_stamp_list = []\n",
    "epoch_list = []\n",
    "for start_time in input_list:\n",
    "    # print(len(start_time[0]))\n",
    "    for dict in start_time[0]:\n",
    "        len_list.append(len(dict))\n",
    "        epoch_list.append(dict)\n",
    "    start_point = start_time[1]\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    time_stamp_list.extend(time_stamps[:-1])\n",
    "len_array = np.array(len_list)\n",
    "time_stamp_array = np.array(time_stamp_list) # 时间序列\n",
    "epoch_array = np.array(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = np.arange(1162393768,1178747998+day_second,day_second)\n",
    "ddf_list = []\n",
    "for start_point in start_list:\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    sub_df_list = []\n",
    "    df = pd.read_hdf('day2.h5',str(start_point))\n",
    "    for i in range(len(time_stamps)-1):\n",
    "        sub_df = df[(df[unix_column] > time_stamps[i]) & (df[unix_column] <= time_stamps[i+1])]\n",
    "        sub_df_list.append(sub_df)\n",
    "    ddf_list.extend(sub_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改为多线程化读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def read_sub_df(start_point):\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    sub_df_list = []\n",
    "    df = pd.read_hdf('day2.h5',str(start_point))\n",
    "    for i in range(len(time_stamps)-1):\n",
    "        sub_df = df[(df[unix_column] > time_stamps[i]) & (df[unix_column] <= time_stamps[i+1])]\n",
    "        sub_df_list.append(sub_df)\n",
    "    print(\"Done \\t {}\".format(start_point))\n",
    "    return (start_point,sub_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done \t 1162825768\n",
      "Done \t 1162480168\n",
      "Done \t 1162393768Done \t 1162998568\n",
      "Done \t 1162652968\n",
      "Done \t 1162739368Done \t 1162566568\n",
      "\n",
      "\n",
      "Done \t 1162912168\n",
      "Done \t 1163171368\n",
      "Done \t 1163516968\n",
      "Done \t 1163257768\n",
      "Done \t 1163344168\n",
      "Done \t 1163430568\n",
      "Done \t 1163603368\n",
      "Done \t 1163689768\n",
      "Done \t 1163084968\n",
      "Done \t 1163776168\n",
      "Done \t 1163862568\n",
      "Done \t 1163948968\n",
      "Done \t 1164035368\n",
      "Done \t 1164121768\n",
      "Done \t 1164208168\n",
      "Done \t 1164294568\n",
      "Done \t 1164380968\n",
      "Done \t 1164467368\n",
      "Done \t 1164553768\n",
      "Done \t 1164640168\n",
      "Done \t 1164726568\n",
      "Done \t 1164812968\n",
      "Done \t 1164899368\n",
      "Done \t 1164985768\n",
      "Done \t 1165072168\n",
      "Done \t 1165158568\n",
      "Done \t 1165244968\n",
      "Done \t 1165331368\n",
      "Done \t 1165417768\n",
      "Done \t 1165504168\n",
      "Done \t 1165590568\n",
      "Done \t 1165676968\n",
      "Done \t 1165763368\n",
      "Done \t 1165849768\n",
      "Done \t 1165936168\n",
      "Done \t 1166022568\n",
      "Done \t 1166108968\n",
      "Done \t 1166195368\n",
      "Done \t 1166281768\n",
      "Done \t 1166368168\n",
      "Done \t 1166454568\n",
      "Done \t 1166540968\n",
      "Done \t 1166627368\n",
      "Done \t 1166713768\n",
      "Done \t 1166800168\n",
      "Done \t 1166886568\n",
      "Done \t 1166972968\n",
      "Done \t 1167059368\n",
      "Done \t 1167145768\n",
      "Done \t 1167232168\n",
      "Done \t 1167318568\n",
      "Done \t 1167404968\n",
      "Done \t 1167491368\n",
      "Done \t 1167577768\n",
      "Done \t 1167664168\n",
      "Done \t 1167750568\n",
      "Done \t 1167836968\n",
      "Done \t 1167923368\n",
      "Done \t 1168009768\n",
      "Done \t 1168096168\n",
      "Done \t 1168182568\n",
      "Done \t 1168268968\n",
      "Done \t 1168355368\n",
      "Done \t 1168441768\n",
      "Done \t 1168528168\n",
      "Done \t 1168614568\n",
      "Done \t 1168700968\n",
      "Done \t 1168787368\n",
      "Done \t 1168873768\n",
      "Done \t 1168960168\n",
      "Done \t 1169046568\n",
      "Done \t 1169132968\n",
      "Done \t 1169219368\n",
      "Done \t 1169305768\n",
      "Done \t 1169392168\n",
      "Done \t 1169478568\n",
      "Done \t 1169564968\n",
      "Done \t 1169651368\n",
      "Done \t 1169737768\n",
      "Done \t 1169824168\n",
      "Done \t 1169910568\n",
      "Done \t 1169996968\n",
      "Done \t 1170083368\n",
      "Done \t 1170169768\n",
      "Done \t 1170256168\n",
      "Done \t 1170342568\n",
      "Done \t 1170428968\n",
      "Done \t 1170515368\n",
      "Done \t 1170601768\n",
      "Done \t 1170688168\n",
      "Done \t 1170774568\n",
      "Done \t 1170860968\n",
      "Done \t 1170947368\n",
      "Done \t 1171033768\n",
      "Done \t 1171120168\n",
      "Done \t 1171206568\n",
      "Done \t 1171292968\n",
      "Done \t 1171379368\n",
      "Done \t 1171465768\n",
      "Done \t 1171552168\n",
      "Done \t 1171638568\n",
      "Done \t 1171724968\n",
      "Done \t 1171811368\n",
      "Done \t 1171897768\n",
      "Done \t 1171984168\n",
      "Done \t 1172070568\n",
      "Done \t 1172156968\n",
      "Done \t 1172329768\n",
      "Done \t 1172243368\n",
      "Done \t 1172416168\n",
      "Done \t 1172502568\n",
      "Done \t 1172588968\n",
      "Done \t 1172675368\n",
      "Done \t 1172761768\n",
      "Done \t 1172848168\n",
      "Done \t 1172934568\n",
      "Done \t 1173020968\n",
      "Done \t 1173107368\n",
      "Done \t 1173193768\n",
      "Done \t 1173280168\n",
      "Done \t 1173366568\n",
      "Done \t 1173452968\n",
      "Done \t 1173539368\n",
      "Done \t 1173625768\n",
      "Done \t 1173712168\n",
      "Done \t 1173798568\n",
      "Done \t 1173884968\n",
      "Done \t 1173971368\n",
      "Done \t 1174057768\n",
      "Done \t 1174144168\n",
      "Done \t 1174230568\n",
      "Done \t 1174316968\n",
      "Done \t 1174403368\n",
      "Done \t 1174489768\n",
      "Done \t 1174576168\n",
      "Done \t 1174662568\n",
      "Done \t 1174748968\n",
      "Done \t 1174835368\n",
      "Done \t 1174921768\n",
      "Done \t 1175008168\n",
      "Done \t 1175094568\n",
      "Done \t 1175180968\n",
      "Done \t 1175267368\n",
      "Done \t 1175353768\n",
      "Done \t 1175440168\n",
      "Done \t 1175526568\n",
      "Done \t 1175612968\n",
      "Done \t 1175699368\n",
      "Done \t 1175785768\n",
      "Done \t 1175872168\n",
      "Done \t 1175958568\n",
      "Done \t 1176044968\n",
      "Done \t 1176131368\n",
      "Done \t 1176217768\n",
      "Done \t 1176304168\n",
      "Done \t 1176390568\n",
      "Done \t 1176476968\n",
      "Done \t 1176563368\n",
      "Done \t 1176649768\n",
      "Done \t 1176736168\n",
      "Done \t 1176822568\n",
      "Done \t 1176908968\n",
      "Done \t 1176995368\n",
      "Done \t 1177081768\n",
      "Done \t 1177168168\n",
      "Done \t 1177254568\n",
      "Done \t 1177340968\n",
      "Done \t 1177427368\n",
      "Done \t 1177513768\n",
      "Done \t 1177772968\n",
      "Done \t 1177600168\n",
      "Done \t 1177686568\n",
      "Done \t 1177859368\n",
      "Done \t 1177945768\n",
      "Done \t 1178032168\n",
      "Done \t 1178118568\n",
      "Done \t 1178204968\n",
      "Done \t 1178291368\n",
      "Done \t 1178377768\n",
      "Done \t 1178464168\n",
      "Done \t 1178550568\n",
      "Done \t 1178809768\n",
      "Done \t 1178636968\n",
      "Done \t 1178723368\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "start_list = np.arange(1162393768,1178747998+day_second,day_second)\n",
    "manager = mp.Manager()\n",
    "# aggreateData = manager.dict()\n",
    "p = Pool(16)\n",
    "tmp_result = []\n",
    "\n",
    "for start_point in start_list:\n",
    "    result = p.apply_async(func=read_sub_df,args=(start_point,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "origin_time = start_list[0]\n",
    "\n",
    "results = []\n",
    "for i in start_list:\n",
    "    results.append([])\n",
    "\n",
    "for item in tmp_result:\n",
    "    temp = item.get()\n",
    "    results[int((temp[0]-origin_time)/86400)].extend(temp[1])\n",
    "\n",
    "ddf_list = []\n",
    "for item in results:\n",
    "    ddf_list.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "index1 = np.where(len_array > 40)[0]\n",
    "maybe_array = epoch_array[index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 116368, 8: 311230, 11: 311649, 14: 297530, 17: 311626, 19: 295072, 23: 309388, 26: 309247, 27: 77054, 31: 299208, 33: 267283, 34: 316424, 35: 274661, 40: 266279, 46: 314496, 47: 114937, 49: 308374, 51: 318062, 55: 138159, 57: 224804, 59: 216308, 60: 292967, 61: 231792, 62: 263384, 63: 208200, 65: 294875, 69: 198539, 70: 316076, 71: 203416, 72: 301465, 73: 306707, 75: 303260, 76: 310711, 79: 273899, 80: 295663, 81: 251044, 82: 131717, 84: 308895, 87: 315132, 88: 234990, 89: 283497, 93: 302051, 96: 313535, 97: 310211, 98: 221392, 100: 306061, 103: 318015, 104: 313801, 109: 190240, 111: 303233, 121: 302922}\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "# init maybe_dict \n",
    "maybe_dict = {}\n",
    "for i in idds_uniq:\n",
    "    maybe_dict[i] = 0\n",
    "\n",
    "\n",
    "for item in maybe_array:\n",
    "    uniq_item = np.unique(item)\n",
    "    for j in uniq_item:\n",
    "        maybe_dict[j] += 1\n",
    "print(maybe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "maybe_dict_sorted = sorted(maybe_dict.items(),key=lambda kv:(kv[1],kv[0]),reverse=True)\n",
    "maybe_dict_sorted = np.array(maybe_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_sensor_list = maybe_dict_sorted.T[0]\n",
    "print(sub_sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "# 测试sensor数量为多少合适\n",
    "def get_sub_index(sensor_count):\n",
    "    sub_sensor_list = idds_uniq_sorted[:sensor_count]\n",
    "    sub_index = []\n",
    "    for i, item in enumerate(epoch_array):\n",
    "        uniq_item = np.unique(item)\n",
    "        diff = np.setdiff1d(sub_sensor_list,uniq_item)\n",
    "        if len(diff) == 0:\n",
    "            sub_index.append(i)\n",
    "    return len(sub_index),sub_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.61964082717896\n"
     ]
    }
   ],
   "source": [
    "#11 多进程版 \n",
    "start_time = time.time()\n",
    "def multi_get_sub_index(i):\n",
    "    return get_sub_index(i)\n",
    "\n",
    "sensor_count_list = np.linspace(10,50,41,dtype=int)\n",
    "\n",
    "manager = mp.Manager()\n",
    "# aggreateData = manager.dict()\n",
    "p = Pool(16)\n",
    "tmp_result = []\n",
    "\n",
    "for i in sensor_count_list:\n",
    "    result = p.apply_async(func=multi_get_sub_index,args=(i,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esmidth/.pyenv/versions/miniconda3-4.7.12/envs/ccs_ta/lib/python3.7/site-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#11-1\n",
    "sub_index_list = []\n",
    "for item in tmp_result:\n",
    "    sub_index_list.append(item.get())\n",
    "sub_index_array = np.array(sub_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#12\n",
    "def get_length_count(window):\n",
    "    for item in sub_index_array:\n",
    "        zeros = np.zeros_like(epoch_array)\n",
    "        zeros[item[1]] = 1\n",
    "        sum = 0\n",
    "        for j in range(len(zeros)):\n",
    "            tmp = zeros[j:j+window]\n",
    "            if 0 not in tmp:\n",
    "                sum += 1\n",
    "        print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-1 DUMP\n",
    "output_file = open('sub_index_array_drop_na.pkl','wb')\n",
    "pickle.dump(sub_index_array,output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-2 LOAD\n",
    "input_file = open('sub_index_array_drop_na.pkl','rb')\n",
    "sub_index_array = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 103, 34, 70, 87, 46, 104, 96, 11, 17, 8, 76, 97, 23, 26, 84, 49, 73, 100]\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "def get_data_matrixs(sensor_count,window,sub_index_array = None):\n",
    "    print(idds_uniq_sorted[:sensor_count])\n",
    "    tmp = sub_index_array[sensor_count-10][1]\n",
    "    zeros = np.zeros_like(epoch_array)\n",
    "    zeros[tmp] = 1\n",
    "\n",
    "    sum_list = []\n",
    "    for i in range(len(zeros)):\n",
    "        tmp1 = zeros[i:i+window]\n",
    "        if 0 not in tmp1:\n",
    "            sum_list.append(i)\n",
    "            \n",
    "    return sum_list\n",
    "indices = get_data_matrixs(19,7,sub_index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#14\n",
    "# 丢弃多余帧\n",
    "def drop_useless_sensor(df,sensor_list):\n",
    "    # print(sensor_list)\n",
    "    for i in df[idd].values:\n",
    "        if i not in sensor_list:\n",
    "            df = df[df[idd] != i]\n",
    "    # print(len(df))\n",
    "    if len(df) < len(sensor_list):\n",
    "        return None\n",
    "    return df\n",
    "# t = drop_useless_sensor(ddf_list[indices[3]],idds_uniq_sorted[:19])\n",
    "# print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "def handle_df(ddf_list,indice,sensor_list):\n",
    "    df = ddf_list[indice]\n",
    "    df = df.drop_duplicates(idd)\n",
    "    df = drop_useless_sensor(df,sensor_list)\n",
    "    return df\n",
    "\n",
    "df_sample = handle_df(ddf_list,indices[0],idds_uniq_sorted[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before 15\n",
    "for i, df in enumerate(ddf_list):\n",
    "    with open('./data/{}.pkl'.format(i),'wb') as f:\n",
    "        pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.412 12.537 12.412 12.537 12.475 12.787 12.787]\n",
      " [13.287 13.35  13.225 13.225 13.1   13.412 13.287]\n",
      " [13.912 14.037 14.037 13.912 13.85  14.1   14.162]\n",
      " [12.662 12.725 12.725 12.725 12.6   12.6   12.725]\n",
      " [13.225 13.537 13.412 13.35  13.35  13.287 13.225]\n",
      " [12.912 12.975 12.85  12.85  12.975 12.912 12.85 ]\n",
      " [11.787 11.662 11.725 11.725 11.787 11.662 11.412]\n",
      " [13.1   12.85  12.975 12.975 12.912 12.85  13.037]\n",
      " [13.475 13.412 13.537 13.537 13.537 13.412 13.662]\n",
      " [12.85  12.975 12.725 12.787 12.85  12.787 12.975]\n",
      " [13.537 13.662 13.85  13.912 13.975 13.787 13.85 ]\n",
      " [13.662 13.787 13.725 13.662 13.725 13.725 13.6  ]\n",
      " [12.662 12.787 12.787 12.85  12.725 12.85  12.662]\n",
      " [13.787 13.85  13.912 13.912 13.912 13.975 13.912]\n",
      " [12.787 12.662 12.85  12.85  12.912 12.975 13.1  ]\n",
      " [13.287 13.287 13.475 13.35  13.35  13.6   13.6  ]\n",
      " [13.287 13.287 13.35  13.162 13.412 13.537 13.475]\n",
      " [13.662 13.662 13.725 13.662 13.662 13.787 13.85 ]\n",
      " [12.475 12.475 12.412 12.475 12.475 12.475 12.475]]\n"
     ]
    }
   ],
   "source": [
    "#15 multi_processing\n",
    "def multi_convert_df_to_dm(indice,sensor_list,window=7):\n",
    "    df_list = []\n",
    "    for i in range(window):\n",
    "        with open('./data/{}.pkl'.format(indice+i),'rb') as f:\n",
    "            sub_df = pickle.load(f)\n",
    "        df_list.append(sub_df)\n",
    "    return_list = []\n",
    "    for df in df_list:\n",
    "        df = df.drop_duplicates(idd)\n",
    "        df = drop_useless_sensor(df,sensor_list)\n",
    "        if type(df) == type(None):\n",
    "            break\n",
    "        df = df.sort_values(by=idd)\n",
    "        flat_matrix = np.zeros(100)\n",
    "        indice_dict = {63:10,23:14,17:15,47:20,7:21,27:22,26:24,14:25,121:26,46:27,8:28,111:32,75:33,57:34,59:35,40:36,34:37,61:41,71:42,65:43,82:45,62:46,76:48,51:51,11:52,70:53,49:54,60:57,84:60,81:61,55:62,72:63,73:64,35:66,31:67,69:68,79:69,103:70,80:71,96:72,97:75,100:76,33:77,19:78,93:79,109:80,87:81,104:84,89:91,88:92,98:94}\n",
    "        return_list.append(df[s_temperature].values)\n",
    "    return np.array(return_list).T\n",
    "res_array = multi_convert_df_to_dm(indices[46],idds_uniq_sorted[:19],7)\n",
    "print(res_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.6   12.412 12.537 12.412 12.537 12.475 12.787]\n",
      " [13.287 13.287 13.35  13.225 13.225 13.1   13.412]\n",
      " [13.787 13.912 14.037 14.037 13.912 13.85  14.1  ]\n",
      " [12.6   12.662 12.725 12.725 12.725 12.6   12.6  ]\n",
      " [13.225 13.225 13.537 13.412 13.35  13.35  13.287]\n",
      " [12.975 12.912 12.975 12.85  12.85  12.975 12.912]\n",
      " [11.787 11.787 11.662 11.725 11.725 11.787 11.662]\n",
      " [12.912 13.1   12.85  12.975 12.975 12.912 12.85 ]\n",
      " [13.287 13.475 13.412 13.537 13.537 13.537 13.412]\n",
      " [12.912 12.85  12.975 12.725 12.787 12.85  12.787]\n",
      " [13.475 13.537 13.662 13.85  13.912 13.975 13.787]\n",
      " [13.85  13.662 13.787 13.725 13.662 13.725 13.725]\n",
      " [12.537 12.662 12.787 12.787 12.85  12.725 12.85 ]\n",
      " [13.725 13.787 13.85  13.912 13.912 13.912 13.975]\n",
      " [12.787 12.787 12.662 12.85  12.85  12.912 12.975]\n",
      " [13.412 13.287 13.287 13.475 13.35  13.35  13.6  ]\n",
      " [13.287 13.287 13.287 13.35  13.162 13.412 13.537]\n",
      " [13.85  13.662 13.662 13.725 13.662 13.662 13.787]\n",
      " [12.35  12.475 12.475 12.412 12.475 12.475 12.475]]\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "def convert_df_to_dm(ddf_list,indice,sensor_list,window=7):\n",
    "    # print(sensor_list)\n",
    "#     print(len(sensor_list))\n",
    "    df_list = []\n",
    "    for i in range(window):\n",
    "        df_list.append(ddf_list[indice+i])\n",
    "#     df = ddf_list[indice]\n",
    "    return_list = []\n",
    "    for df in df_list:\n",
    "        df = df.drop_duplicates(idd)\n",
    "#         print(indice,type(df))\n",
    "#         print(len(df))\n",
    "        df = drop_useless_sensor(df,sensor_list)\n",
    "        if type(df) == type(None):\n",
    "            break\n",
    "        df = df.sort_values(by=idd)\n",
    "        flat_matrix = np.zeros(100)\n",
    "        indice_dict = {63:10,23:14,17:15,47:20,7:21,27:22,26:24,14:25,121:26,46:27,8:28,111:32,75:33,57:34,59:35,40:36,34:37,61:41,71:42,65:43,82:45,62:46,76:48,51:51,11:52,70:53,49:54,60:57,84:60,81:61,55:62,72:63,73:64,35:66,31:67,69:68,79:69,103:70,80:71,96:72,97:75,100:76,33:77,19:78,93:79,109:80,87:81,104:84,89:91,88:92,98:94}\n",
    "        return_list.append(df[s_temperature].values)\n",
    "#         for i,indice in enumerate(df[idd].values):\n",
    "#             flat_matrix[indice_dict[indice]] = df[s_temperature].values[i]\n",
    "    return np.array(return_list).T\n",
    "    \n",
    "\n",
    "res_array = convert_df_to_dm(ddf_list,indices[45],idds_uniq_sorted[:19],7)\n",
    "print(res_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(ddf_list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592.4611942768097\n"
     ]
    }
   ],
   "source": [
    "#16\n",
    "start_time = time.time()\n",
    "array_list2 = []\n",
    "for i in indices:\n",
    "    array_list2.append(convert_df_to_dm(ddf_list,i,idds_uniq_sorted[:19],7))\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.80443000793457\n"
     ]
    }
   ],
   "source": [
    "#16多线程\n",
    "array_list2 = []\n",
    "manager = mp.Manager()\n",
    "\n",
    "p = Pool(16)\n",
    "\n",
    "start_time = time.time()\n",
    "def multi_get_sub_index(i):\n",
    "    return get_sub_index(i)\n",
    "\n",
    "tmp_result = []\n",
    "\n",
    "for i in indices:\n",
    "    result = p.apply_async(func=multi_convert_df_to_dm,args=(i,idds_uniq_sorted[:19],7,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "for item in tmp_result:\n",
    "    array_list2.append(item.get())\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28681\n",
      "28682\n",
      "28683\n",
      "28684\n",
      "28690\n",
      "28691\n",
      "28692\n",
      "28720\n",
      "28768\n",
      "28769\n",
      "28770\n",
      "28771\n",
      "28772\n",
      "28842\n",
      "28851\n",
      "28852\n",
      "28853\n",
      "29068\n",
      "29069\n",
      "29143\n",
      "29144\n",
      "29233\n",
      "29234\n",
      "29261\n",
      "29304\n",
      "29305\n",
      "29330\n",
      "29404\n",
      "29405\n",
      "29406\n",
      "29407\n",
      "29408\n",
      "29409\n",
      "29410\n",
      "29449\n",
      "29450\n",
      "29451\n",
      "29452\n",
      "29453\n",
      "29454\n",
      "29455\n",
      "29557\n",
      "29558\n",
      "29559\n",
      "29560\n",
      "29561\n",
      "29604\n",
      "29605\n",
      "29606\n",
      "29640\n",
      "29641\n",
      "29665\n",
      "29673\n",
      "29674\n",
      "29794\n",
      "29795\n",
      "29882\n",
      "29883\n",
      "29884\n",
      "29885\n",
      "29886\n",
      "29887\n",
      "29888\n",
      "29890\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "new_indices = []\n",
    "for i,item in enumerate(array_list2):\n",
    "    if np.isnan(item).any():\n",
    "        print(i)\n",
    "#         sum+=1\n",
    "    else:\n",
    "        new_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1162393768, 1162393798, 1162393828, ..., 1178896078, 1178896108,\n",
       "       1178896138])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_stamp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4_tensor.pkl','wb') as f:\n",
    "    pickle.dump(array_list2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4_time_stamp_array.pkl','wb') as f:\n",
    "    pickle.dump(time_stamp_array,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4_indices_na.pkl','wb') as f:\n",
    "    pickle.dump(indices,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('4_indices_no_na.pkl','wb') as f:\n",
    "    pickle.dump(new_indices,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
