{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from consts import sensor_day_seconds as day_second\n",
    "from consts import sensor_idd as idd\n",
    "from consts import sensor_idds_uniq as idds_uniq\n",
    "from consts import sensor_idds_uniq_sorted as idds_uniq_sorted\n",
    "from consts import sensor_a_temperature as a_temperature\n",
    "from consts import sensor_s_temperature as s_temperature\n",
    "from consts import sensor_unix_column as unix_column\n",
    "from consts import sensor_idd as idd\n",
    "epoch_length = 30\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "input_file_new = open('3_dict_list_new.pkl','rb')\n",
    "input_list_new = pickle.load(input_file_new)\n",
    "input_file_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# 将epoch epoch_len epoch_time 都flat化\n",
    "len_list_new = []\n",
    "time_stamp_list_new = []\n",
    "epoch_list_new = []\n",
    "for start_time in input_list_new:\n",
    "    # print(len(start_time[0]))\n",
    "    for dict in start_time[0]:\n",
    "        len_list_new.append(len(dict))\n",
    "        epoch_list_new.append(dict)\n",
    "    start_point = start_time[1]\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    time_stamp_list_new.extend(time_stamps[:-1])\n",
    "len_array_new = np.array(len_list_new)\n",
    "time_stamp_array_new = np.array(time_stamp_list_new) # 时间序列\n",
    "epoch_array_new = np.array(epoch_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = np.arange(1162393768,1178747998+day_second,day_second)\n",
    "ddf_list_new = []\n",
    "for start_point in start_list:\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    sub_df_list = []\n",
    "    df = pd.read_hdf('day2.h5',str(start_point))\n",
    "    for i in range(len(time_stamps)-1):\n",
    "        sub_df = df[(df[unix_column] > time_stamps[i]) & (df[unix_column] <= time_stamps[i+1])]\n",
    "        sub_df_list.append(sub_df)\n",
    "    ddf_list_new.extend(sub_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改为多线程化读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def read_sub_df(start_point):\n",
    "    time_stamps = np.arange(start_point,start_point+day_second+epoch_length,epoch_length)\n",
    "    sub_df_list = []\n",
    "    df = pd.read_hdf('day2_sa.h5',str(start_point))\n",
    "    for i in range(len(time_stamps)-1):\n",
    "        sub_df = df[(df[unix_column] > time_stamps[i]) & (df[unix_column] <= time_stamps[i+1])]\n",
    "        \n",
    "        sub_df_list.append(sub_df)\n",
    "    print(\"Done \\t {}\".format(start_point))\n",
    "    return (start_point,sub_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done \t 1162825768\n",
      "Done \t 1162739368\n",
      "Done \t 1162652968\n",
      "Done \t 1162480168\n",
      "Done \t 1162566568\n",
      "Done \t 1162912168\n",
      "Done \t 1163171368\n",
      "Done \t 1162393768\n",
      "Done \t 1163084968\n",
      "Done \t 1162998568\n",
      "Done \t 1163344168\n",
      "Done \t 1163430568\n",
      "Done \t 1163257768Done \t 1163603368\n",
      "\n",
      "Done \t 1163516968\n",
      "Done \t 1163689768\n",
      "Done \t 1163862568\n",
      "Done \t 1163776168\n",
      "Done \t 1163948968\n",
      "Done \t 1164035368\n",
      "Done \t 1164121768\n",
      "Done \t 1164208168\n",
      "Done \t 1164294568\n",
      "Done \t 1164380968\n",
      "Done \t 1164467368\n",
      "Done \t 1164553768\n",
      "Done \t 1164640168\n",
      "Done \t 1164726568\n",
      "Done \t 1164812968\n",
      "Done \t 1164899368\n",
      "Done \t 1164985768\n",
      "Done \t 1165072168\n",
      "Done \t 1165158568\n",
      "Done \t 1165244968\n",
      "Done \t 1165331368\n",
      "Done \t 1165417768\n",
      "Done \t 1165504168\n",
      "Done \t 1165590568\n",
      "Done \t 1165676968\n",
      "Done \t 1165763368\n",
      "Done \t 1165849768\n",
      "Done \t 1165936168\n",
      "Done \t 1166022568\n",
      "Done \t 1166108968\n",
      "Done \t 1166195368\n",
      "Done \t 1166281768\n",
      "Done \t 1166368168\n",
      "Done \t 1166454568\n",
      "Done \t 1166540968\n",
      "Done \t 1166627368\n",
      "Done \t 1166713768\n",
      "Done \t 1166800168\n",
      "Done \t 1166886568\n",
      "Done \t 1166972968\n",
      "Done \t 1167059368\n",
      "Done \t 1167145768\n",
      "Done \t 1167232168\n",
      "Done \t 1167318568\n",
      "Done \t 1167404968\n",
      "Done \t 1167491368\n",
      "Done \t 1167577768\n",
      "Done \t 1167664168\n",
      "Done \t 1167750568\n",
      "Done \t 1167836968\n",
      "Done \t 1167923368\n",
      "Done \t 1168009768\n",
      "Done \t 1168096168\n",
      "Done \t 1168182568\n",
      "Done \t 1168268968\n",
      "Done \t 1168355368\n",
      "Done \t 1168441768\n",
      "Done \t 1168528168\n",
      "Done \t 1168614568\n",
      "Done \t 1168700968\n",
      "Done \t 1168787368\n",
      "Done \t 1168873768\n",
      "Done \t 1168960168\n",
      "Done \t 1169046568\n",
      "Done \t 1169132968\n",
      "Done \t 1169219368\n",
      "Done \t 1169305768\n",
      "Done \t 1169478568\n",
      "Done \t 1169392168\n",
      "Done \t 1169564968\n",
      "Done \t 1169651368\n",
      "Done \t 1169737768\n",
      "Done \t 1169824168\n",
      "Done \t 1169910568\n",
      "Done \t 1169996968\n",
      "Done \t 1170083368\n",
      "Done \t 1170169768\n",
      "Done \t 1170256168\n",
      "Done \t 1170342568\n",
      "Done \t 1170428968\n",
      "Done \t 1170515368\n",
      "Done \t 1170601768\n",
      "Done \t 1170688168\n",
      "Done \t 1170774568\n",
      "Done \t 1170860968\n",
      "Done \t 1170947368\n",
      "Done \t 1171033768\n",
      "Done \t 1171120168\n",
      "Done \t 1171206568\n",
      "Done \t 1171292968\n",
      "Done \t 1171379368\n",
      "Done \t 1171465768\n",
      "Done \t 1171552168\n",
      "Done \t 1171638568\n",
      "Done \t 1171724968\n",
      "Done \t 1171811368\n",
      "Done \t 1171897768\n",
      "Done \t 1171984168\n",
      "Done \t 1172070568\n",
      "Done \t 1172156968\n",
      "Done \t 1172329768\n",
      "Done \t 1172243368\n",
      "Done \t 1172416168\n",
      "Done \t 1172502568\n",
      "Done \t 1172588968\n",
      "Done \t 1172675368\n",
      "Done \t 1172761768\n",
      "Done \t 1172848168\n",
      "Done \t 1172934568\n",
      "Done \t 1173020968\n",
      "Done \t 1173107368\n",
      "Done \t 1173193768\n",
      "Done \t 1173280168\n",
      "Done \t 1173366568\n",
      "Done \t 1173452968\n",
      "Done \t 1173539368\n",
      "Done \t 1173625768\n",
      "Done \t 1173712168\n",
      "Done \t 1173798568\n",
      "Done \t 1173884968\n",
      "Done \t 1173971368\n",
      "Done \t 1174057768\n",
      "Done \t 1174144168\n",
      "Done \t 1174230568\n",
      "Done \t 1174316968\n",
      "Done \t 1174403368\n",
      "Done \t 1174489768\n",
      "Done \t 1174576168\n",
      "Done \t 1174662568\n",
      "Done \t 1174748968\n",
      "Done \t 1174835368\n",
      "Done \t 1174921768\n",
      "Done \t 1175008168\n",
      "Done \t 1175094568\n",
      "Done \t 1175180968\n",
      "Done \t 1175267368\n",
      "Done \t 1175353768\n",
      "Done \t 1175440168\n",
      "Done \t 1175526568\n",
      "Done \t 1175612968\n",
      "Done \t 1175699368\n",
      "Done \t 1175785768\n",
      "Done \t 1175872168\n",
      "Done \t 1175958568\n",
      "Done \t 1176044968\n",
      "Done \t 1176131368\n",
      "Done \t 1176217768\n",
      "Done \t 1176304168\n",
      "Done \t 1176390568\n",
      "Done \t 1176476968\n",
      "Done \t 1176563368\n",
      "Done \t 1176649768\n",
      "Done \t 1176736168\n",
      "Done \t 1176822568\n",
      "Done \t 1176908968\n",
      "Done \t 1176995368\n",
      "Done \t 1177081768\n",
      "Done \t 1177168168\n",
      "Done \t 1177254568\n",
      "Done \t 1177340968\n",
      "Done \t 1177427368\n",
      "Done \t 1177513768\n",
      "Done \t 1177772968\n",
      "Done \t 1177600168\n",
      "Done \t 1177686568\n",
      "Done \t 1177859368\n",
      "Done \t 1177945768\n",
      "Done \t 1178032168\n",
      "Done \t 1178118568\n",
      "Done \t 1178204968\n",
      "Done \t 1178291368\n",
      "Done \t 1178377768\n",
      "Done \t 1178464168\n",
      "Done \t 1178550568\n",
      "Done \t 1178809768\n",
      "Done \t 1178636968\n",
      "Done \t 1178723368\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "start_list = np.arange(1162393768,1178747998+day_second,day_second)\n",
    "manager = mp.Manager()\n",
    "# aggreateData = manager.dict()\n",
    "p = Pool(16)\n",
    "tmp_result = []\n",
    "\n",
    "for start_point in start_list:\n",
    "    result = p.apply_async(func=read_sub_df,args=(start_point,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "origin_time = start_list[0]\n",
    "\n",
    "results = []\n",
    "for i in start_list:\n",
    "    results.append([])\n",
    "\n",
    "for item in tmp_result:\n",
    "    temp = item.get()\n",
    "    results[int((temp[0]-origin_time)/86400)].extend(temp[1])\n",
    "\n",
    "ddf_list_new = []\n",
    "for item in results:\n",
    "    ddf_list_new.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "473651\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "index1 = np.where(len_array_new > 40)[0]\n",
    "maybe_array_new = epoch_array_new[index1]\n",
    "print(len(maybe_array_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{7: 125799, 8: 471452, 11: 458757, 14: 472821, 17: 458104, 19: 445912, 23: 465393, 26: 464479, 27: 84998, 31: 433210, 33: 459727, 34: 472749, 35: 473617, 40: 455014, 46: 466469, 47: 129074, 49: 451279, 51: 444594, 55: 151859, 57: 251149, 59: 266859, 60: 429722, 61: 259182, 62: 315350, 63: 247709, 65: 353682, 69: 240795, 70: 466843, 71: 246772, 72: 467454, 73: 464136, 75: 375903, 76: 413628, 79: 456291, 80: 425423, 81: 379307, 82: 187571, 84: 437322, 87: 450836, 88: 356917, 89: 447802, 93: 450445, 96: 460671, 97: 468705, 98: 328718, 100: 452027, 103: 442609, 104: 444617, 109: 234528, 111: 362542, 121: 473651}\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "# init maybe_dict \n",
    "maybe_dict = {}\n",
    "for i in idds_uniq:\n",
    "    maybe_dict[i] = 0\n",
    "\n",
    "\n",
    "for item in maybe_array_new:\n",
    "    uniq_item = np.unique(item)\n",
    "    for j in uniq_item:\n",
    "        maybe_dict[j] += 1\n",
    "print(maybe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9                                                      \n",
    "maybe_dict_sorted = sorted(maybe_dict.items(),key=lambda kv:(kv[1],kv[0]),reverse=True)\n",
    "maybe_dict_sorted = np.array(maybe_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[121  35  14  34   8  97  72  70  46  23  26  73  96  33  11  17  79  40\n 100  49  87  93  89  19 104  51 103  84  31  60  80  76  81  75 111  88\n  65  98  62  59  61  57  63  71  69 109  82  55  47   7  27]\n"
     ]
    }
   ],
   "source": [
    "sub_sensor_list = maybe_dict_sorted.T[0]\n",
    "print(sub_sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "# 测试sensor数量为多少合适\n",
    "def get_sub_index(sensor_count):\n",
    "    sub_sensor_list = idds_uniq_sorted[:sensor_count]\n",
    "    sub_index = []\n",
    "    for i, item in enumerate(epoch_array_new):\n",
    "        uniq_item = np.unique(item)\n",
    "        diff = np.setdiff1d(sub_sensor_list,uniq_item)\n",
    "        if len(diff) == 0:\n",
    "            sub_index.append(i)\n",
    "    return len(sub_index),sub_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(550080,)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "epoch_array_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "id 10 number: 378683\nid 11 number: 378487\nid 12 number: 348561\nid 13 number: 345626\nid 14 number: 344499\nid 15 number: 343160\nid 16 number: 330523\nid 17 number: 327771\nid 18 number: 327440\nid 19 number: 311179\nid 20 number: 278013\nid 21 number: 272067\nid 22 number: 272067\nid 23 number: 262523\nid 24 number: 262258\nid 25 number: 240924\nid 26 number: 240666\nid 27 number: 223298\nid 28 number: 209368\nid 29 number: 192740\nid 30 number: 183545\nid 31 number: 177827\nid 32 number: 177825\nid 33 number: 172067\nid 34 number: 171416\nid 35 number: 170802\nid 36 number: 137159\nid 37 number: 112691\nid 38 number: 104700\nid 39 number: 65497\nid 40 number: 63813\nid 41 number: 49961\nid 42 number: 46459\nid 43 number: 9169\nid 44 number: 9112\nid 45 number: 9101\nid 46 number: 9026\nid 47 number: 4103\nid 48 number: 887\nid 49 number: 0\nid 50 number: 0\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(sub_index_array):\n",
    "    print(\"id\",i+10,\"number:\",item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "len(sub_index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "78.22364854812622\n"
     ]
    }
   ],
   "source": [
    "#11 多进程版 \n",
    "start_time = time.time()\n",
    "def multi_get_sub_index(i):\n",
    "    return get_sub_index(i)\n",
    "\n",
    "sensor_count_list = np.linspace(10,50,41,dtype=int)\n",
    "\n",
    "manager = mp.Manager()\n",
    "# aggreateData = manager.dict()\n",
    "p = Pool(16)\n",
    "tmp_result = []\n",
    "\n",
    "for i in sensor_count_list:\n",
    "    result = p.apply_async(func=multi_get_sub_index,args=(i,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "#11-1\n",
    "sub_index_list = []\n",
    "for item in tmp_result:\n",
    "    sub_index_list.append(item.get())\n",
    "sub_index_array = np.array(sub_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(41, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "sub_index_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# obsolete 12 \n",
    "def get_length_count(window):\n",
    "    for item in sub_index_array:\n",
    "        zeros = np.zeros_like(epoch_array)\n",
    "        zeros[item[1]] = 1\n",
    "        sum = 0\n",
    "        for j in range(len(zeros)):\n",
    "            tmp = zeros[j:j+window]\n",
    "            if 0 not in tmp:\n",
    "                sum += 1\n",
    "        print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-1 DUMP\n",
    "output_file = open('sub_index_array_drop_na_sa.pkl','wb')\n",
    "pickle.dump(sub_index_array,output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-2 LOAD\n",
    "input_file = open('sub_index_array_drop_na_sa.pkl','rb')\n",
    "sub_index_array = pickle.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[51, 103, 34, 70, 87, 46, 104, 96, 11, 17, 8, 76, 97, 23, 26, 84, 49, 73, 100]\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "def get_data_matrixs(sensor_count,window,sub_index_array = None):\n",
    "    print(idds_uniq_sorted[:sensor_count])\n",
    "    tmp = sub_index_array[sensor_count-10][1]\n",
    "    zeros = np.zeros_like(epoch_array_new)\n",
    "    zeros[tmp] = 1\n",
    "\n",
    "    sum_list = []\n",
    "    for i in range(len(zeros)):\n",
    "        tmp1 = zeros[i:i+window]\n",
    "        if 0 not in tmp1:\n",
    "            sum_list.append(i)\n",
    "            \n",
    "    return sum_list\n",
    "indices = get_data_matrixs(19,7,sub_index_array)\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "226922"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "95141"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "indices[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#14\n",
    "# 丢弃多余帧\n",
    "def drop_useless_sensor(df,sensor_list):\n",
    "    # print(sensor_list)\n",
    "    for i in df[idd].values:\n",
    "        if i not in sensor_list:\n",
    "            df = df[df[idd] != i]\n",
    "    # print(len(df))\n",
    "    if len(df) < len(sensor_list):\n",
    "        return None\n",
    "    return df\n",
    "# t = drop_useless_sensor(ddf_list[indices[3]],idds_uniq_sorted[:19])\n",
    "# print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "def handle_df(ddf_list,indice,sensor_list):\n",
    "    df = ddf_list[indice]\n",
    "    df = df.drop_duplicates(idd)\n",
    "    df = drop_useless_sensor(df,sensor_list)\n",
    "    return df\n",
    "\n",
    "df_sample = handle_df(ddf_list,indices[0],idds_uniq_sorted[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before 15\n",
    "for i, df in enumerate(ddf_list):\n",
    "    with open('./data/{}.pkl'.format(i),'wb') as f:\n",
    "        pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 multi_processing\n",
    "def multi_convert_df_to_dm(indice,sensor_list,window=7):\n",
    "    df_list = []\n",
    "    for i in range(window):\n",
    "        with open('./data/{}.pkl'.format(indice+i),'rb') as f:\n",
    "            sub_df = pickle.load(f)\n",
    "        df_list.append(sub_df)\n",
    "    return_list = []\n",
    "    for df in df_list:\n",
    "        df = df.drop_duplicates(idd)\n",
    "        df = drop_useless_sensor(df,sensor_list)\n",
    "        if type(df) == type(None):\n",
    "            break\n",
    "        df = df.sort_values(by=idd)\n",
    "        flat_matrix = np.zeros(100)\n",
    "        indice_dict = {63:10,23:14,17:15,47:20,7:21,27:22,26:24,14:25,121:26,46:27,8:28,111:32,75:33,57:34,59:35,40:36,34:37,61:41,71:42,65:43,82:45,62:46,76:48,51:51,11:52,70:53,49:54,60:57,84:60,81:61,55:62,72:63,73:64,35:66,31:67,69:68,79:69,103:70,80:71,96:72,97:75,100:76,33:77,19:78,93:79,109:80,87:81,104:84,89:91,88:92,98:94}\n",
    "        return_list.append(df[s_temperature].values)\n",
    "    return np.array(return_list).T\n",
    "res_array = multi_convert_df_to_dm(indices[46],idds_uniq_sorted[:19],7)\n",
    "print(res_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#15\n",
    "def convert_df_to_dm(ddf_list,indice,sensor_list,window=7):\n",
    "    # print(sensor_list)\n",
    "#     print(len(sensor_list))\n",
    "    df_list = []\n",
    "    for i in range(window):\n",
    "        df_list.append(ddf_list[indice+i])\n",
    "#     df = ddf_list[indice]\n",
    "    return_list = []\n",
    "    for df in df_list:\n",
    "        df = df.drop_duplicates(idd)\n",
    "#         print(indice,type(df))\n",
    "#         print(len(df))\n",
    "        df = drop_useless_sensor(df,sensor_list)\n",
    "        if type(df) == type(None):\n",
    "            break\n",
    "        df = df.sort_values(by=idd)\n",
    "        flat_matrix = np.zeros(100)\n",
    "        indice_dict = {63:10,23:14,17:15,47:20,7:21,27:22,26:24,14:25,121:26,46:27,8:28,111:32,75:33,57:34,59:35,40:36,34:37,61:41,71:42,65:43,82:45,62:46,76:48,51:51,11:52,70:53,49:54,60:57,84:60,81:61,55:62,72:63,73:64,35:66,31:67,69:68,79:69,103:70,80:71,96:72,97:75,100:76,33:77,19:78,93:79,109:80,87:81,104:84,89:91,88:92,98:94}\n",
    "        return_list.append(df[s_temperature].values)\n",
    "#         for i,indice in enumerate(df[idd].values):\n",
    "#             flat_matrix[indice_dict[indice]] = df[s_temperature].values[i]\n",
    "    return np.array(return_list).T\n",
    "    \n",
    "\n",
    "res_array = convert_df_to_dm(ddf_list,indices[45],idds_uniq_sorted[:19],7)\n",
    "print(res_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(ddf_list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16多线程\n",
    "array_list2 = []\n",
    "manager = mp.Manager()\n",
    "\n",
    "p = Pool(16)\n",
    "\n",
    "start_time = time.time()\n",
    "def multi_get_sub_index(i):\n",
    "    return get_sub_index(i)\n",
    "\n",
    "tmp_result = []\n",
    "\n",
    "for i in indices:\n",
    "    result = p.apply_async(func=multi_convert_df_to_dm,args=(i,idds_uniq_sorted[:19],7,))\n",
    "    tmp_result.append(result)\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "for item in tmp_result:\n",
    "    array_list2.append(item.get())\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "start_time = time.time()\n",
    "array_list2 = []\n",
    "for i in indices:\n",
    "    array_list2.append(convert_df_to_dm(ddf_list,i,idds_uniq_sorted[:19],7))\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before 17-4\n",
    "sum = 0\n",
    "new_indices = []\n",
    "for i,item in enumerate(array_list2):\n",
    "    if np.isnan(item).any():\n",
    "        print(i)\n",
    "#         sum+=1\n",
    "    else:\n",
    "        new_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17-1\n",
    "with open('4_tensor.pkl','wb') as f:\n",
    "    pickle.dump(array_list2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17-2\n",
    "with open('4_time_stamp_array.pkl','wb') as f:\n",
    "    pickle.dump(time_stamp_array,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17-3\n",
    "with open('4_indices_na.pkl','wb') as f:\n",
    "    pickle.dump(indices,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17-4\n",
    "with open('4_indices_no_na.pkl','wb') as f:\n",
    "    pickle.dump(new_indices,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd0906b79bf097d1f1d9673c7efb6a8c75b3b37c36b1c482a31cb0779974e6343ce",
   "display_name": "Python 3.7.9 64-bit ('ccs_ta': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}